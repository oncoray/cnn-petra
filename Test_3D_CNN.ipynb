{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path \n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "from matplotlib import pyplot as plt\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "from tensorflow import keras\n",
    "from sklearn import metrics\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines import KaplanMeierFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DenseNet3D\n",
    "import ResNet3D\n",
    "import Vgg3D\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = nib.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize by 10\"\"\"\n",
    "    volume[volume > 10] = 10\n",
    "    volume = volume/10\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    # Normalize\n",
    "    volume = normalize(volume)\n",
    "    return volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, x_test,i, weights):\n",
    "    fname = \"3d_image_cox_reg_\"+ str(i) + \".hdf5\"\n",
    "    fname=os.path.join(weights,fname)\n",
    "    model.load_weights(fname)\n",
    "\n",
    "    prediction_test = []\n",
    "    for i in range(0,len(x_test)):\n",
    "        prediction = model.predict(np.expand_dims(np.expand_dims(x_test[i], axis=3), axis=0))[0]\n",
    "        prediction_test.append(prediction[0])\n",
    "    return prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci(y_test, yhat_test):\n",
    "    CI_test  = concordance_index(y_test[:,1], yhat_test,y_test[:,0])\n",
    "    return CI_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory with training weights\n",
    "weights = Path (r'D:\\Transcend_(E)\\_IramS\\PETra_ML_DL_Analysis\\Deep_learning\\Experiments_Cox_PET\\Exp_18\\Weights')\n",
    "# path to directory to store test results \n",
    "Exp_dir = Path(r'D:\\Transcend_(E)\\_IramS\\PETra_ML_DL_Analysis\\Deep_learning\\Experiments_Cox_PET\\Exp_18')\n",
    "Exp_dir = os.path.join(Exp_dir,'Test')\n",
    "# create test directory\n",
    "if not os.path.exists(Exp_dir):\n",
    "    os.mkdir(Exp_dir)\n",
    "os.chdir(Exp_dir)\n",
    "\n",
    "# File with patient ids and labels \n",
    "Test_org= pd.read_csv(r'D:\\Transcend_(E)\\_IramS\\PETra_ML_DL_Analysis\\Processed_Features\\PET_Features_Validation\\PET_PETra_Clinical_Imaging_Validation_Features_Robust.csv')\n",
    "\n",
    "# Path to test image patches stored in nifti format \n",
    "Test_org['Path']= 'D:\\Transcend_(E)\\_IramS\\PETra_ML_DL_Analysis\\Deep_learning\\Data\\PETra_Cox_model_data\\PETra_Cox_model_Validation'+'\\\\'+ Test_org.subject_id+'_0_base_img.nii.gz'\n",
    "Test_org = Test_org[['subject_id','LC','LCtime','Path']]\n",
    "Test_org=Test_org.drop(labels=12, axis=0)\n",
    "X_test_subvolumes= Test_org.rename(columns={'LC': 'status', 'LCtime': 'time'}) # rename the columns for easy interprestation in remaining script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data \n",
    "splits=5\n",
    "repeats=5\n",
    "CI_test_all=[]  # empty list for storing CI at each iteration \n",
    "risk_ensemble_test =pd.DataFrame()\n",
    "risk_ensemble_test['subject_id'] = X_test_subvolumes.subject_id # store all subject ids \n",
    "\n",
    "x_test = np.array([process_scan(path) for path in X_test_subvolumes.Path])\n",
    "y_test = np.array(X_test_subvolumes[['status','time']])\n",
    "\n",
    "######### Load saved model ############################\n",
    "model = DenseNet3D.DenseNet3DImageNet121(input_shape=(60,60,44,1), include_top=False)\n",
    "\n",
    "#Model compilation is not required while loading weights to model \n",
    "#https://stackoverflow.com/questions/41859997/keras-model-load-weights-for-neural-net\n",
    "#https://stackoverflow.com/questions/47995324/does-model-compile-initialize-all-the-weights-and-biases-in-keras-tensorflow\n",
    "#######################################################\n",
    "for i in range (1,(splits*repeats)+1):\n",
    "    yhat_test = evaluate_test(model, x_test, i, weights) \n",
    "# ##################### collect Subject_risk in current fold for computing Ensemble risk ##############################\n",
    "# for test apply all 25 models and save the results in ensemble \n",
    "    risk_ensemble_test['risk_fold_'+str(i)] = yhat_test\n",
    "    ci_test_all = compute_ci(y_test,yhat_test)\n",
    "    CI_test_all.append(ci_test_all)\n",
    "    \n",
    "    print('########## CI Train at fold %d is %.2f ############' % (i,(ci_test_all)))\n",
    "i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the ensemble risk for test and validation \n",
    "risk_ensemble_test.fillna(0)\n",
    "risk_ensemble_test['Ensemble'] = risk_ensemble_test.loc[:,'risk_fold_1': 'risk_fold_'+str(splits*repeats)].mean(axis=1)\n",
    "#Calculate CI from ensemble risk \n",
    "risk_ensemble_test['status'],risk_ensemble_test['time'] = X_test_subvolumes['status'],X_test_subvolumes['time']\n",
    "CI_ensemble_test  = concordance_index(risk_ensemble_test.time,risk_ensemble_test.Ensemble,risk_ensemble_test.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogRank_test(risk_ensemble):\n",
    "    pred= logrank_test(np.array(risk_ensemble.loc[risk_ensemble.risk_group==0].time),\n",
    "                       np.array(risk_ensemble.loc[risk_ensemble.risk_group==1].time),\n",
    "                       np.array(risk_ensemble.loc[risk_ensemble.risk_group==0].status),\n",
    "                       np.array(risk_ensemble.loc[risk_ensemble.risk_group==1].status),\n",
    "                       alpha=0.95)\n",
    "    p_value = pred.p_value\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the optimal cutoff selected on training data\n",
    "optimal_cutoff= 0.015\n",
    "\n",
    "#Valid p-values at optimal threshold using log-rank test \n",
    "\n",
    "risk_ensemble_test['risk_group'] = np.where(risk_ensemble_test['Ensemble'] > optimal_cutoff, 1,0)\n",
    "p_val_test= LogRank_test(risk_ensemble_test)\n",
    "risk_ensemble_test.to_excel('Ensemble_test_risk_at_optimal_cutoff.xlsx', index=None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Test KM\n",
    "size=16\n",
    "data_test= risk_ensemble_test\n",
    "\n",
    "ix= data_test[\"risk_group\"] ==1\n",
    "\n",
    "#plotting images#\n",
    "plt.rc('xtick', labelsize=size)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=size)\n",
    "fig,ax = plt.subplots(nrows=1, ncols=1,figsize= (7,7))\n",
    "\n",
    "#testing kaplan meier\n",
    "kmf_test_high = KaplanMeierFitter()\n",
    "ax = kmf_test_high.fit(data_test.loc[ix][\"time\"], data_test.loc[ix]['status'], label='High risk').plot(ax=ax,\n",
    "                            show_censors=True,ci_show=False, xlim=(0,60), color= \"blue\",yticks=(0.0,0.2,0.4,0.6,0.8,1),\n",
    "                            ylim=(0.0,1),xticks=(0,12,24,36,48,60))\n",
    "\n",
    "kmf_test_low = KaplanMeierFitter()\n",
    "ax = kmf_test_low.fit(data_test.loc[~ix][\"time\"], data_test.loc[~ix]['status'], label='Low risk').plot(ax=ax,\n",
    "                             show_censors=True,ci_show=False, xlim=(0,60),color=\"orange\",yticks=(0.0,0.2,0.4,0.6,0.8,1),\n",
    "                             ylim=(0.0,1),xticks=(0,12,24,36,48,60))\n",
    "\n",
    "\n",
    "#settings for testing km\n",
    "ax.set_xlabel(\"Time [months]\",size=size)\n",
    "ax.set_ylabel(\"Local Recurrence\",size=size)\n",
    "ax.text(45, 0.25, \"p=\"+str(round(p_val_test,3)), size=size)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=size)\n",
    "\n",
    "ax.legend(fontsize=size, loc=1)\n",
    "ax.set_title(\"Independent Validation\", size=size)\n",
    "\n",
    "\n",
    "from lifelines.plotting import add_at_risk_counts\n",
    "util.add_at_risk(kmf_test_high, kmf_test_low, ax=ax,rows_to_show=['At risk'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Test_starta.png\", dpi=300)\n",
    "#dpi stands for dots per pixel per inch, good quality is above 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout = open(\"Test_Results.txt\", \"w\")\n",
    "print(\"Median CI Test (All Subvolumes) %.2f\" % np.median(CI_test_all))\n",
    "print('######################################')\n",
    "print(\"CI Ensemble Test (All Subvolumes) %.2f\" % CI_ensemble_test)\n",
    "print(\"p_value at optimal cutoff %.3f on test data %.5f\" % (optimal_cutoff, p_val_test))\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-invitation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
